---
title: "Project Code"
author: "Gunjan"
date: "22 November 2017"
output: html_document
---

#Load libraries.

```{r, warning=FALSE}
library(tidyverse)
library(caret)
```

#Load data.

```{r}
mash_data <- read.csv("/Users/abhijithasok/Documents/Harvard_Health_Data_Science/Fall_2017/Intro to Data Science/Project//OnlineNewsPopularity/OnlineNewsPopularity.csv",header = T,colClasses = c('character',rep('numeric',12),rep('factor',6),rep('numeric',12),rep('factor',8),rep('numeric',22)))

mash_work <- mash_data

scraped_data <- read.csv("/Users/abhijithasok/Documents/Harvard_Health_Data_Science/Fall_2017/Intro to Data Science/Project/ScrapedDataFull.csv",header = T)

scraped_data$X.1 <- NULL
scraped_data$X <- NULL
```

#Adding IDs

```{r}
id <- seq(1,nrow(mash_work),1)
mash_work <- cbind(id,mash_work)
```

#Adding scraped data

```{r}
mash_work <- mash_work %>% left_join(scraped_data[,c("id","date","author","title")],by="id")
```

#Assumption - All authors have written more than one article. Remove authors with just 1 article from the data.

```{r}
article_count_by_author <- mash_work %>%
                            group_by(author) %>%
                            dplyr::summarise(author_article_count = n())

mash_work <- mash_work %>% left_join(article_count_by_author,by="author")

mash_work <- mash_work %>% filter(author_article_count > 1)
```

#Split into train and test.

```{r}
set.seed(10)
train_Index <- createDataPartition(mash_work$shares, times = 1, p=0.9, list = F)
mash_work_train <- mash_work[train_Index,]
mash_work_test <- mash_work[-train_Index,]
```

#Outlier Removal

```{r}
mash_work_train <- mash_work_train %>% filter(shares < 150000) #Outlier removal
share_mean <- mean(mash_work_train$shares)
```

#Variable trends

```{r}
mash_work_train %>% ggplot(aes(x=(global_rate_positive_words),y=shares)) + geom_point() + geom_abline()
colSums(mash_work_train == 0)/nrow(mash_work_train) * 100
```

#Baseline prediction with share mean.

```{r}
share_mean <- mean(mash_work_train$shares)
prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = share_mean)

RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

#Author effect - No regularization

```{r}
author_means <- mash_work_train %>% 
                  group_by(author) %>%
                  dplyr::summarise(author_effect = mean(shares - share_mean))

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, author = mash_work_test$author)

prediction_list <- prediction_list %>% inner_join(author_means,by="author")

prediction_list <- prediction_list %>%
                    mutate(predicted_shares = share_mean + author_effect)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Author effect - Regularization

```{r}
# lambdas <- seq(0,1000)
# tmp <- mash_work_train %>% 
#   group_by(author) %>% 
#   summarize(author_effect = sum(shares - share_mean), n_i = n())
# 
# rmses <- sapply(lambdas, function(l){
#   joined <- mash_work_test %>% 
#     left_join(tmp, by='author') %>% 
#     mutate(b_i = author_effect/(n_i+l)) %>%
#     replace_na(list(b_i=0))
#     predicted_shares <- share_mean + joined$b_i
#     return(RMSE(predicted_shares, mash_work_test$shares))
# })
# qplot(lambdas, rmses) 
```

# Text effects - Top words with TF > 300 ( Note io to ios)

```{r}
top_words <- c("twitter", "facebook", "app", "appl", "googl", "iphon", "amazon", "video", "obama", "photo", "smartphon", "instagram", "microsoft", "android", "mobil", "samsung", "youtub", "vine", "ipad", "million", "trailer", "internet", "ios", "tech")

mash_work_train$title <- tolower(mash_work_train$title)
mash_work_test$title <- tolower(mash_work_test$title)

mash_work_train <- mash_work_train %>%
                    mutate(top_words_title = ifelse(grepl(paste(top_words,collapse = "|"),title),1,0))

mash_work_test <- mash_work_test %>%
                    mutate(top_words_title = ifelse(grepl(paste(top_words,collapse = "|"),title),1,0))
```

#Topword effect

```{r}
shares_by_topwords <- mash_work_train %>%
                        group_by(top_words_title) %>%
                        dplyr::summarise(topword_effect = mean(shares - share_mean))

prediction_list <- prediction_list %>% left_join(mash_work_test[,c("id","top_words_title")],by="id")

prediction_list <- prediction_list %>% left_join(shares_by_topwords,by="top_words_title")

prediction_list <- prediction_list %>%
                    mutate(predicted_shares = share_mean + topword_effect)

RMSE(prediction_list$shares,prediction_list$predicted_shares)

```

# Clearing unused character and factor variables for modelling

```{r}
mash_work_train <- mash_work_train[,-c(2:3,21:29,41:45)]
mash_work_test <- mash_work_test[,-c(2:3,21:29,41:45)]
mash_work_train$author_article_count <- as.numeric(mash_work_train$author_article_count)
mash_work_test$author_article_count <- as.numeric(mash_work_test$author_article_count)
```

#Baseline regression

```{r, warning=FALSE}
mash_work_train_selected <- mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))]

mash_work_test_selected <- mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))]

# tofactor <- mash_work_train_selected[,c(10:15,17:22,32)]
# tofactor <- data.frame(apply(tofactor, 2, as.factor))
# mash_work_train_selected <- cbind(mash_work_train_selected[,-c(10:15,17:22,32)],tofactor)
# 
# tofactor <- mash_work_test_selected[,c(10:15,17:22,32)]
# tofactor <- data.frame(apply(tofactor, 2, as.factor))
# mash_work_test_selected <- cbind(mash_work_test_selected[,-c(10:15,17:22,32)],tofactor)

regmodel <- lm(shares ~ ., data=mash_work_train_selected,na.action = na.omit)
predictions <- predict(regmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
```

#Removing near zero variance variables

```{r}
sds <- apply(mash_work_train_selected, 2, function(x)sd(x))
lapply(sds,function(x)x<1)
```


#Baseline regression without correlated variables

```{r}
# regmodel <- lm(shares ~ .- n_unique_tokens - n_non_stop_words - self_reference_min_shares - self_reference_max_shares - global_sentiment_polarity - weekday_is_saturday - weekday_is_sunday - title_subjectivity - rate_negative_words, data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))])
# predictions <- predict(regmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])
# 
# prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)
# 
# RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

#Step-wise forward

```{r}
library(olsrr)
olsforward <- ols_step_forward(regmodel)

#removed_vars <- olsback$removed

regmodel1 <- lm(as.formula(paste("shares ~ ",paste(olsforward$predictors,collapse = "+"),sep="")), data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))])
predictions <- predict(regmodel1, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
```

#CV RF

```{r}
set.seed(10)
library(randomForest)
library(doMC)
registerDoMC(cores = 6)
# system.time(res <- train(as.formula(paste("shares ~ ",paste(olsforward$predictors,collapse = "+"),sep="")),
#              data = (mash_work_train_selected),
#               method = "rf",
#               ntree = 500,
#               maxnodes = 20,
# #              nodesize = 20,
#               tuneGrid = expand.grid(.mtry=round(ncol(mash_work_train_selected))/3),
#               trControl = trainControl(method='repeatedcv', number=3, repeats=3, allowParallel = TRUE),
#               metric="RMSE"))


system.time(res <- train(shares ~ .,
             data = (mash_work_train_selected),
              method = "rf",
              ntree = 100,
              maxnodes = 20,
#              nodesize = 20,
              tuneGrid = expand.grid(.mtry=round(ncol(mash_work_train_selected)/2)),
              trControl = trainControl(method='repeatedcv', number=3, repeats=3,allowParallel = TRUE),
              metric="RMSE"))

predictions <- predict(res, newdata = mash_work_test_selected)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
```

#CV XGBoost

```{r}
set.seed(10)
library(randomForest)
library(doMC)
registerDoMC(cores = 8)
# system.time(res <- train(as.formula(paste("shares ~ ",paste(olsforward$predictors,collapse = "+"),sep="")),
#              data = (mash_work_train_selected),
#               method = "rf",
#               ntree = 500,
#               maxnodes = 20,
# #              nodesize = 20,
#               tuneGrid = expand.grid(.mtry=round(ncol(mash_work_train_selected))/3),
#               trControl = trainControl(method='repeatedcv', number=3, repeats=3, allowParallel = TRUE),
#               metric="RMSE"))

parametersGrid <-  expand.grid(eta = 0.01, 
                            colsample_bytree=c(0.5,0.7),
                            max_depth=c(4,5),
                            nrounds=1500,
                            gamma=1,
                            min_child_weight=2,
                            subsample=0.75
                            )
set.seed(10)
system.time(res <- train(shares ~ .,
             data = (mash_work_train_selected),
              method = "xgbTree",
              ntree = 10,
              maxnodes = 20,
#              nodesize = 20,
              tuneGrid = parametersGrid,
              trControl = trainControl(method='repeatedcv', number=3, repeats=3,allowParallel = TRUE),
              metric="RMSE"))

predictions <- predict(res, newdata = mash_work_test_selected)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
```










































# Ridge Regression

```{r}
library(glmnet)

mash_work_selected <- mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))]

mash_work_selected_test <- mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))]

mash_work_selected$shares <- as.numeric(mash_work_selected$shares)
lambdas <- 10^seq(3, -2, by = -.1)
cv_fit <- cv.glmnet(as.matrix(mash_work_selected[,-30]), mash_work_selected[,30], alpha = 0, lambda = lambdas)
plot(cv_fit)
opt_lambda <- cv_fit$lambda.min
fit <- glmnet(as.matrix(mash_work_selected[,-30]), mash_work_selected[,30], alpha = 0, lambda = opt_lambda)

predictions <- predict(fit, newx = as.matrix(mash_work_selected_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))]),s=opt_lambda)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
```































#PCA

```{r}
prin_comp_train <- prcomp(mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))], scale. = T)
pr_var <- (prin_comp$sdev)^2
prop_varex <- pr_var / sum(pr_var)
plot(prop_varex, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b")

plot(cumsum(prop_varex), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

# We choose 30 PCs. They cover 94.5% of the variance of the data

comp_train <- data.frame(prin_comp_train$x)[,1:30]

# Doing the same thing over the test set.

prin_comp_test <- prcomp(mash_work_test[,-which(names(mash_work_test) %in% c("id","shares","date","author","title"))], scale. = T)
pr_var <- (prin_comp$sdev)^2
prop_varex <- pr_var / sum(pr_var)
plot(prop_varex, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b")

plot(cumsum(prop_varex), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

comp_test <- data.frame(prin_comp_test$x)[,1:30]
```

# Regression with PCs

```{r}
comp_train <- cbind(comp_train,shares = mash_work_train$shares)
comp_test <- cbind(comp_test,shares = mash_work_test$shares)

regmodel <- lm(shares ~ ., data=comp_train)
predictions <- predict(regmodel, newdata = comp_test)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Decision Tree with PCs

```{r}
library(rpart)
treemodel <- rpart(shares ~., data=comp_train)
plot(treemodel)
predictions <- predict(treemodel, newdata = comp_test)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Decision Tree with actual variables

```{r}
treemodel <- rpart(shares ~., data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))])
plot(treemodel)
predictions <- predict(treemodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Random Forest Baseline

```{r}
library(randomForest)
forestmodel <- randomForest(shares ~ average_token_length + num_hrefs + num_self_hrefs + n_tokens_content + data_channel_is_tech + data_channel_is_world + self_reference_avg_sharess, data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))], ntree = 100)
plot(forestmodel)
predictions <- predict(forestmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Choosing best variables

```{r}
library(tidyverse)
var_percents <- row.names(data.frame(prin_comp_train$rotation))
best_vars_prin_comp <-  data.frame(prin_comp_train$rotation) %>%
                        mutate(vars = row.names(data.frame(prin_comp_train$rotation))) %>%
                        filter(PC1 >= 0.2) 
```

# RF with best variables

```{r}
forestmodel <- randomForest(shares ~ average_token_length + global_subjectivity + global_sentiment_polarity + global_rate_positive_words + rate_positive_words + avg_positive_polarity + max_positive_polarity , data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))], ntree = 100)
plot(forestmodel)
predictions <- predict(forestmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

