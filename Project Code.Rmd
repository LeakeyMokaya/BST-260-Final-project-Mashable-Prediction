---
title: "Project Code"
author: "Gunjan"
date: "22 November 2017"
output: html_document
---

#Load libraries.

```{r, warning=FALSE}
library(tidyverse)
library(caret)
```

#Load data.

```{r}
mash_data <- read.csv("/Users/abhijithasok/Documents/Harvard_Health_Data_Science/Fall_2017/Intro to Data Science/Project//OnlineNewsPopularity/OnlineNewsPopularity.csv",header = T,colClasses = c('character',rep('numeric',12),rep('factor',6),rep('numeric',12),rep('factor',8),rep('numeric',22)))

mash_work <- mash_data

scraped_data <- read.csv("/Users/abhijithasok/Documents/Harvard_Health_Data_Science/Fall_2017/Intro to Data Science/Project/ScrapedDataFull.csv",header = T)

scraped_data$X.1 <- NULL
scraped_data$X <- NULL
```

#Adding IDs

```{r}
id <- seq(1,nrow(mash_work),1)
mash_work <- cbind(id,mash_work)
```

#Adding scraped data

```{r}
mash_work <- mash_work %>% left_join(scraped_data[,c("id","date","author","title")],by="id")
```

#Assumption - All authors have written more than one article. Remove authors with just 1 article from the data.

```{r}
article_count_by_author <- mash_work %>%
                            group_by(author) %>%
                            dplyr::summarise(author_article_count = n())

mash_work <- mash_work %>% left_join(article_count_by_author,by="author")

mash_work <- mash_work %>% filter(author_article_count > 1)
```

#Split into train and test.

```{r}
set.seed(10)
train_Index <- createDataPartition(mash_work$shares, times = 1, p=0.9, list = F)
mash_work_train <- mash_work[train_Index,]
mash_work_test <- mash_work[-train_Index,]
```

#Outlier Removal

```{r}
mash_work_train <- mash_work_train %>% filter(shares < 150000) #Outlier removal
share_mean <- mean(mash_work_train$shares)
#write.csv(mash_work_train,"Final train full.csv",row.names = FALSE)
```

#Baseline prediction with share mean.

```{r}
rmse_list <- data.frame(method = character(),rmse = numeric(),stringsAsFactors = FALSE)
share_mean <- mean(mash_work_train$shares)
prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = share_mean)

RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}

RMSE(prediction_list$shares,prediction_list$predicted_shares)

c=1
rmse_list[c,] <- c("Uniform prediction with mean of all shares",RMSE(prediction_list$shares,prediction_list$predicted_shares))
c<-c+1
```

#Author effect - No regularization

```{r}
author_means <- mash_work_train %>% 
                  group_by(author) %>%
                  dplyr::summarise(author_effect = mean(shares - share_mean))

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, author = mash_work_test$author)

prediction_list <- prediction_list %>% inner_join(author_means,by="author")

prediction_list <- prediction_list %>%
                    mutate(predicted_shares = share_mean + author_effect)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
rmse_list[c,] <- c("Prediction with Mean and Author effect",RMSE(prediction_list$shares,prediction_list$predicted_shares))
c<-c+1 
```

# Processing of Title text

```{r}
library(tm)

articles <- mash_work_train

#Converting to corpus
article_corpus <- Corpus(VectorSource(articles$title))

#Converting to lower case
article_corpus <- tm_map(article_corpus, content_transformer(tolower))

#Remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
article_corpus <- tm_map(article_corpus, content_transformer(removeURL))

#Keep only English letters and space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
article_corpus <- tm_map(article_corpus, content_transformer(removeNumPunct))

#Remove stopwords
myStopwords <- c(stopwords('english'), "mashable")
article_corpus <- tm_map(article_corpus, removeWords, myStopwords)

#Remove extra whitespace
article_corpus <- tm_map(article_corpus, stripWhitespace)

#Copy for stem completion
article_corpus_copy <- article_corpus

#Stem Document
article_corpus <- tm_map(article_corpus, content_transformer(stemDocument))

#Document - Term Matrix
tdm <- TermDocumentMatrix(article_corpus,
                          control = list(wordLengths = c(1, Inf),weighting = weightTfIdf))

#Frequent terms
freq.terms <- findFreqTerms(tdm, lowfreq = 300)
```


# Text effects - Top words with TF > 300 ( Note io to ios)

```{r}
top_words <- c("twitter", "facebook", "app", "appl", "googl", "iphon", "amazon", "video", "obama", "photo", "smartphon", "instagram", "microsoft", "android", "mobil", "samsung", "youtub", "vine", "ipad", "million", "trailer", "internet", "ios", "tech")

mash_work_train$title <- tolower(mash_work_train$title)
mash_work_test$title <- tolower(mash_work_test$title)

mash_work_train <- mash_work_train %>%
                    mutate(top_words_title = ifelse(grepl(paste(top_words,collapse = "|"),title),1,0))

mash_work_test <- mash_work_test %>%
                    mutate(top_words_title = ifelse(grepl(paste(top_words,collapse = "|"),title),1,0))
```

#Topword effect

```{r}
shares_by_topwords <- mash_work_train %>%
                        group_by(top_words_title) %>%
                        dplyr::summarise(topword_effect = mean(shares - share_mean))

prediction_list <- prediction_list %>% left_join(mash_work_test[,c("id","top_words_title")],by="id")

prediction_list <- prediction_list %>% left_join(shares_by_topwords,by="top_words_title")

prediction_list <- prediction_list %>%
                    mutate(predicted_shares = share_mean + topword_effect)

RMSE(prediction_list$shares,prediction_list$predicted_shares)

rmse_list[c,] <- c("Prediction with Mean and topwords effect",RMSE(prediction_list$shares,prediction_list$predicted_shares))
c<-c+1
```

# Clearing unused character and factor variables for modelling

```{r}
mash_work_train <- mash_work_train[,-c(2:3,21:29,41:45)]
mash_work_test <- mash_work_test[,-c(2:3,21:29,41:45)]
mash_work_train$author_article_count <- as.numeric(mash_work_train$author_article_count)
mash_work_test$author_article_count <- as.numeric(mash_work_test$author_article_count)
```

#Baseline regression

```{r, warning=FALSE}
mash_work_train_selected <- mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))]

mash_work_test_selected <- mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))]

# tofactor <- mash_work_train_selected[,c(10:15,17:22,32)]
# tofactor <- data.frame(apply(tofactor, 2, as.factor))
# mash_work_train_selected <- cbind(mash_work_train_selected[,-c(10:15,17:22,32)],tofactor)
# 
# tofactor <- mash_work_test_selected[,c(10:15,17:22,32)]
# tofactor <- data.frame(apply(tofactor, 2, as.factor))
# mash_work_test_selected <- cbind(mash_work_test_selected[,-c(10:15,17:22,32)],tofactor)

regmodel <- lm(shares ~ ., data=mash_work_train_selected,na.action = na.omit)
predictions <- predict(regmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
rmse_list[c,] <- c("Regression with all variables after correlation treatment",RMSE(prediction_list$shares,prediction_list$predicted_shares))
c<-c+1

# cooks <- data.frame(id = mash_work_train_selected$id,dist = cooks.distance(regmodel),shares=mash_work_train_selected$shares)
# 
# pot_out <- cooks %>% filter(dist > 3*mean(cooks$dist))
# 
# mash_work_train_selected <- mash_work_train_selected %>% filter(!(id %in% pot_out$id))
```

#Step-wise forward

```{r}
library(olsrr)
olsforward <- ols_step_forward(regmodel)

#removed_vars <- olsback$removed

regmodel1 <- lm(as.formula(paste("shares ~ ",paste(olsforward$predictors,collapse = "+"),sep="")), data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))])
predictions <- predict(regmodel1, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
rmse_list[c,] <- c("Step-wise forward regression",RMSE(prediction_list$shares,prediction_list$predicted_shares))
c<-c+1
```

#CV RF

```{r}
set.seed(10)
library(randomForest)
library(doMC)
registerDoMC(cores = 6)
# system.time(res <- train(as.formula(paste("shares ~ ",paste(olsforward$predictors,collapse = "+"),sep="")),
#              data = (mash_work_train_selected),
#               method = "rf",
#               ntree = 500,
#               maxnodes = 20,
# #              nodesize = 20,
#               tuneGrid = expand.grid(.mtry=round(ncol(mash_work_train_selected))/3),
#               trControl = trainControl(method='repeatedcv', number=3, repeats=3, allowParallel = TRUE),
#               metric="RMSE"))


system.time(res <- train(shares ~ .,
             data = (mash_work_train_selected),
              method = "rf",
              ntree = 100,
              maxnodes = 20,
#              nodesize = 20,
              tuneGrid = expand.grid(.mtry=round(ncol(mash_work_train_selected)/2)),
              trControl = trainControl(method='repeatedcv', number=3, repeats=3,allowParallel = TRUE),
              metric="RMSE"))

predictions <- predict(res, newdata = mash_work_test_selected)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
rmse_list[c,] <- c("Tuned Random Forest with repeated cross-validation and parallel processing",RMSE(prediction_list$shares,prediction_list$predicted_shares))
c<-c+1
```

#CV XGBoost

```{r}
set.seed(10)
library(xgboost)
library(doMC)
registerDoMC(cores = 8)
# system.time(res <- train(as.formula(paste("shares ~ ",paste(olsforward$predictors,collapse = "+"),sep="")),
#              data = (mash_work_train_selected),
#               method = "rf",
#               ntree = 500,
#               maxnodes = 20,
# #              nodesize = 20,
#               tuneGrid = expand.grid(.mtry=round(ncol(mash_work_train_selected))/3),
#               trControl = trainControl(method='repeatedcv', number=3, repeats=3, allowParallel = TRUE),
#               metric="RMSE"))

parametersGrid <-  expand.grid(eta = 0.005, 
                            colsample_bytree=c(0.5,0.7),
                            max_depth=c(4,5),
                            nrounds=2000,
                            gamma=1,
                            min_child_weight=2,
                            subsample=0.75
                            )
set.seed(10)
system.time(res <- train(shares ~ .,
             data = (mash_work_train_selected),
              method = "xgbTree",
              ntree = 10,
              maxnodes = 20,
#              nodesize = 20,
              tuneGrid = parametersGrid,
              verbose = TRUE,
              trControl = trainControl(method='repeatedcv', number=3, repeats=3,allowParallel = TRUE),
              metric="RMSE"))

predictions <- predict(res, newdata = mash_work_test_selected)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
rmse_list[c,] <- c("Tuned Gradient Boosted Trees with repeated cross-validation and parallel processing",RMSE(prediction_list$shares,prediction_list$predicted_shares))
c<-c+1
```

#RMSE table format and save

```{r}
rmse_list$rmse <- as.numeric(rmse_list$rmse)
rmse_list <- rmse_list %>% mutate(rmse = round(rmse,digits=2))
nos <- seq(1,nrow(rmse_list),1)
algos <- c("Mean","Mean + Author","Mean + Topwords","Linear Regression","Step-wise Forward Linear Regression","Tuned Random Forest","Tuned Gradient Boosted Trees")
rmse_list <- cbind(nos,algos,rmse_list)
write.csv(rmse_list,"Mashable_shares_prediction/RMSEs.csv",row.names = FALSE)
```


#h2o

```{r}
# library(h2o)
# localH2O <- h2o.init(min_mem_size = "20g")
# train_h2o <- as.h2o(mash_work_train_selected, key="train_h2o")
# test_h2o <- as.h2o(mash_work_test_selected, key="train_h2o")
# model <- h2o.deeplearning( x = c(1:29,31:32),  # column numbers for predictors
#                y = 20,   # column number for label
#                training_frame = train_h2o, # data in H2O format
#                stopping_metric = "RMSE",
#                activation = "TanhWithDropout", # or 'Tanh'
#                input_dropout_ratio = 0.2, # % of inputs dropout
#                hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
#                balance_classes = TRUE, 
#                hidden = c(50,50,50), # three layers of 50 nodes
#                epochs = 100)
# 
# predictions <- h2o.predict(model, newdata = test_h2o)
# 
# prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)
# 
# ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
 ```