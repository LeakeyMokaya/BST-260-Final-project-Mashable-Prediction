---
title: "Project Code"
author: "Gunjan"
date: "22 November 2017"
output: html_document
---

#Load libraries.

```{r, warning=FALSE}
library(tidyverse)
library(caret)
```

#Load data.

```{r}
mash_data <- read.csv("/Users/abhijithasok/Documents/Harvard Health Data Science/Fall 2017/Intro to Data Science/Project//OnlineNewsPopularity/OnlineNewsPopularity.csv",header = T)

mash_work <- mash_data

scraped_data <- read.csv("/Users/abhijithasok/Documents/Harvard Health Data Science/Fall 2017/Intro to Data Science/Project/ScrapedDataFull.csv",header = T)

scraped_data$X.1 <- NULL
scraped_data$X <- NULL
```

#Adding IDs

```{r}
id <- seq(1,nrow(mash_work),1)
mash_work <- cbind(id,mash_work)
```


#Adding scraped data

```{r}
mash_work <- mash_work %>% inner_join(scraped_data[,c("id","date","author","title")],by="id")
```

#Outlier Removal

```{r}
mash_work %>% ggplot(aes(x=id,y=shares)) + geom_boxplot()
mash_work <- mash_work %>% filter(shares < 250000) #Outlier removal
mash_work %>% ggplot(aes(x=id,y=shares)) + geom_boxplot()
share_mean <- mean(mash_work$shares)
```

#Assumption - All authors have written more than one article. Remove authors with just 1 article from the data.

```{r}
article_count_by_author <- mash_work %>%
                            group_by(author) %>%
                            summarise(author_article_count = n())

mash_work <- mash_work %>% left_join(article_count_by_author,by="author")

mash_work <- mash_work %>% filter(author_article_count > 1)

```


#Split into train and test.

```{r}
set.seed(10)
train_Index <- createDataPartition(mash_work$shares, times = 1, p=0.75, list = F)
mash_work_train <- mash_work[train_Index,]
mash_work_test <- mash_work[-train_Index,]
```

#Baseline prediction with share mean.

```{r}
share_mean <- mean(mash_work_train$shares)
prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = share_mean)

RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

#Author effect - No regularization

```{r}
author_means <- mash_work_train %>% 
                  group_by(author) %>%
                  summarise(author_effect = mean(shares - share_mean))

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, author = mash_work_test$author)

prediction_list <- prediction_list %>% inner_join(author_means,by="author")

prediction_list <- prediction_list %>%
                    mutate(predicted_shares = share_mean + author_effect)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Author effect - Regularization

```{r}
# lambdas <- seq(0,1000)
# tmp <- mash_work_train %>% 
#   group_by(author) %>% 
#   summarize(author_effect = sum(shares - share_mean), n_i = n())
# 
# rmses <- sapply(lambdas, function(l){
#   joined <- mash_work_test %>% 
#     left_join(tmp, by='author') %>% 
#     mutate(b_i = author_effect/(n_i+l)) %>%
#     replace_na(list(b_i=0))
#     predicted_shares <- share_mean + joined$b_i
#     return(RMSE(predicted_shares, mash_work_test$shares))
# })
# qplot(lambdas, rmses) 
```

# Text effects - Top words with TF > 300 ( Note io to ios)

```{r}
top_words <- c("twitter", "facebook", "app", "appl", "googl", "iphon", "amazon", "video", "obama", "photo", "smartphon", "instagram", "microsoft", "android", "mobil", "samsung", "youtub", "vine", "ipad", "million", "trailer", "internet", "ios", "tech")

mash_work_train$title <- tolower(mash_work_train$title)
mash_work_test$title <- tolower(mash_work_test$title)

mash_work_train <- mash_work_train %>%
                    mutate(top_words_title = ifelse(grepl(paste(top_words,collapse = "|"),title),1,0))

mash_work_test <- mash_work_test %>%
                    mutate(top_words_title = ifelse(grepl(paste(top_words,collapse = "|"),title),1,0))
```

#Topword effect

```{r}
shares_by_topwords <- mash_work_train %>%
                        group_by(top_words_title) %>%
                        summarise(topword_effect = mean(shares - share_mean))

prediction_list <- prediction_list %>% left_join(mash_work_test[,c("id","top_words_title")],by="id")

prediction_list <- prediction_list %>% left_join(shares_by_topwords,by="top_words_title")

prediction_list <- prediction_list %>%
                    mutate(predicted_shares = share_mean + topword_effect)

RMSE(prediction_list$shares,prediction_list$predicted_shares)

```

# Clearing unused character and factor variables for modelling

```{r}
mash_work_train <- mash_work_train[,-c(2:3,21:29,41:45)]
mash_work_test <- mash_work_test[,-c(2:3,21:29,41:45)]
mash_work_train$author_article_count <- as.numeric(mash_work_train$author_article_count)
mash_work_test$author_article_count <- as.numeric(mash_work_test$author_article_count)
```

#Baseline regression

```{r}
regmodel <- lm(shares ~ ., data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))],na.action = na.omit)
predictions <- predict(regmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
```

#Baseline regression without correlated variables

```{r}
# regmodel <- lm(shares ~ .- n_unique_tokens - n_non_stop_words - self_reference_min_shares - self_reference_max_shares - global_sentiment_polarity - weekday_is_saturday - weekday_is_sunday - title_subjectivity - rate_negative_words, data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))])
# predictions <- predict(regmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])
# 
# prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)
# 
# RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

#Step-wise backward

```{r}
library(olsrr)
olsback <- ols_step_forward(regmodel)

#removed_vars <- olsback$removed

regmodel1 <- lm(as.formula(paste("shares ~ ",paste(olsback$predictors,collapse = "+"),sep="")), data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))])
predictions <- predict(regmodel1, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

ModelMetrics::rmse(prediction_list$shares,prediction_list$predicted_shares)
```

#CV

```{r}
res <- train(as.formula(paste("shares ~ ",paste(olsback$predictors,collapse = "+"),sep="")),
             data = mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title","n_unique_tokens","n_non_stop_words","self_reference_min_shares","self_reference_max_shares","global_sentiment_polarity","weekday_is_saturday","weekday_is_sunday","title_subjectivity","rate_negative_words","max_positive_polarity","min_positive_polarity","max_negative_polarity","min_negative_polarity","global_subjectivity","rate_positive_words"))],
             method = "rf",
             trControl = trainControl(method='cv', number=20),
             metric="RMSE")
```


#PCA

```{r}
prin_comp_train <- prcomp(mash_work_train[,-which(names(mash_work_train) %in% c("id","shares","date","author","title"))], scale. = T)
pr_var <- (prin_comp$sdev)^2
prop_varex <- pr_var / sum(pr_var)
plot(prop_varex, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b")

plot(cumsum(prop_varex), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

# We choose 30 PCs. They cover 94.5% of the variance of the data

comp_train <- data.frame(prin_comp_train$x)[,1:30]

# Doing the same thing over the test set.

prin_comp_test <- prcomp(mash_work_test[,-which(names(mash_work_test) %in% c("id","shares","date","author","title"))], scale. = T)
pr_var <- (prin_comp$sdev)^2
prop_varex <- pr_var / sum(pr_var)
plot(prop_varex, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b")

plot(cumsum(prop_varex), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

comp_test <- data.frame(prin_comp_test$x)[,1:30]
```

# Regression with PCs

```{r}
comp_train <- cbind(comp_train,shares = mash_work_train$shares)
comp_test <- cbind(comp_test,shares = mash_work_test$shares)

regmodel <- lm(shares ~ ., data=comp_train)
predictions <- predict(regmodel, newdata = comp_test)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Decision Tree with PCs

```{r}
library(rpart)
treemodel <- rpart(shares ~., data=comp_train)
plot(treemodel)
predictions <- predict(treemodel, newdata = comp_test)

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Decision Tree with actual variables

```{r}
treemodel <- rpart(shares ~., data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))])
plot(treemodel)
predictions <- predict(treemodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Random Forest Baseline

```{r}
library(randomForest)
forestmodel <- randomForest(shares ~ average_token_length + num_hrefs + num_self_hrefs + n_tokens_content + data_channel_is_tech + data_channel_is_world + self_reference_avg_sharess, data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))], ntree = 100)
plot(forestmodel)
predictions <- predict(forestmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

# Choosing best variables

```{r}
library(tidyverse)
var_percents <- row.names(data.frame(prin_comp_train$rotation))
best_vars_prin_comp <-  data.frame(prin_comp_train$rotation) %>%
                        mutate(vars = row.names(data.frame(prin_comp_train$rotation))) %>%
                        filter(PC1 >= 0.2) 
```

# RF with best variables

```{r}
forestmodel <- randomForest(shares ~ average_token_length + global_subjectivity + global_sentiment_polarity + global_rate_positive_words + rate_positive_words + avg_positive_polarity + max_positive_polarity , data=mash_work_train[,-which(names(mash_work_train) %in% c("id","date","author","title"))], ntree = 100)
plot(forestmodel)
predictions <- predict(forestmodel, newdata = mash_work_test[,-which(names(mash_work_test) %in% c("id","date","author","title"))])

prediction_list <- data.frame(id = mash_work_test$id, shares = mash_work_test$shares, predicted_shares = predictions)

RMSE(prediction_list$shares,prediction_list$predicted_shares)
```

